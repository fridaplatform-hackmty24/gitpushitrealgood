{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3829 files belonging to 6 classes.\n",
      "Using 3064 files for training.\n",
      "Found 3829 files belonging to 6 classes.\n",
      "Using 765 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 08:59:42.803727: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-09-15 08:59:42.803746: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-09-15 08:59:42.803760: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-09-15 08:59:42.803773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-15 08:59:42.803782: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 08:59:43.348941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - accuracy: 0.2431 - loss: 1.9382 - val_accuracy: 0.4340 - val_loss: 1.3661 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.4803 - loss: 1.3175 - val_accuracy: 0.5595 - val_loss: 1.1159 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.6032 - loss: 1.0979 - val_accuracy: 0.6118 - val_loss: 1.0124 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.6362 - loss: 0.9516 - val_accuracy: 0.6392 - val_loss: 0.8888 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.6847 - loss: 0.8267 - val_accuracy: 0.6641 - val_loss: 0.8520 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.7575 - loss: 0.6432 - val_accuracy: 0.6784 - val_loss: 0.9203 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.7923 - loss: 0.5765 - val_accuracy: 0.7098 - val_loss: 0.8010 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.8403 - loss: 0.4783 - val_accuracy: 0.7242 - val_loss: 0.8487 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.8745 - loss: 0.3595 - val_accuracy: 0.7425 - val_loss: 0.7511 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.8945 - loss: 0.3047 - val_accuracy: 0.7634 - val_loss: 0.7866 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9147 - loss: 0.2652 - val_accuracy: 0.7608 - val_loss: 0.8192 - learning_rate: 0.0010\n",
      "Epoch 12/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9275 - loss: 0.2120\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9275 - loss: 0.2119 - val_accuracy: 0.7634 - val_loss: 0.8979 - learning_rate: 0.0010\n",
      "Epoch 13/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.9525 - loss: 0.1470 - val_accuracy: 0.7752 - val_loss: 0.7900 - learning_rate: 5.0000e-04\n",
      "Epoch 14/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.9715 - loss: 0.0965 - val_accuracy: 0.7647 - val_loss: 0.9721 - learning_rate: 5.0000e-04\n",
      "Epoch 15/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9662 - loss: 0.0953\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9662 - loss: 0.0953 - val_accuracy: 0.7477 - val_loss: 1.0220 - learning_rate: 5.0000e-04\n",
      "Epoch 16/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9701 - loss: 0.0752 - val_accuracy: 0.7686 - val_loss: 0.9957 - learning_rate: 2.5000e-04\n",
      "Epoch 17/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9800 - loss: 0.0570 - val_accuracy: 0.7712 - val_loss: 0.9558 - learning_rate: 2.5000e-04\n",
      "Epoch 18/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9823 - loss: 0.0482\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9823 - loss: 0.0481 - val_accuracy: 0.7778 - val_loss: 0.9912 - learning_rate: 2.5000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.9827 - loss: 0.0513 - val_accuracy: 0.7699 - val_loss: 1.0348 - learning_rate: 1.2500e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.9851 - loss: 0.0421 - val_accuracy: 0.7856 - val_loss: 1.0060 - learning_rate: 1.2500e-04\n",
      "Epoch 21/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9813 - loss: 0.0541\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9814 - loss: 0.0541 - val_accuracy: 0.7843 - val_loss: 1.0108 - learning_rate: 1.2500e-04\n",
      "Epoch 22/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0361 - val_accuracy: 0.7830 - val_loss: 1.0288 - learning_rate: 6.2500e-05\n",
      "Epoch 23/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9866 - loss: 0.0381 - val_accuracy: 0.7765 - val_loss: 1.0189 - learning_rate: 6.2500e-05\n",
      "Epoch 24/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9853 - loss: 0.0382\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.9853 - loss: 0.0382 - val_accuracy: 0.7843 - val_loss: 1.0275 - learning_rate: 6.2500e-05\n",
      "Epoch 25/25\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.9920 - loss: 0.0307 - val_accuracy: 0.7843 - val_loss: 1.0392 - learning_rate: 3.1250e-05\n",
      "24/24 - 0s - 13ms/step - accuracy: 0.7843 - loss: 1.0392\n",
      "INFO:tensorflow:Assets written to: rice_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rice_final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'rice_final'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  6165488528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165490832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165491216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165493136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165494096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165493520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6165494864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "path = 'Rice_Leaf_AUG'\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32  \n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Store class_names before applying operations\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor the validation loss to adjust the learning rate\n",
    "    factor=0.5,          # Reduce the learning rate by a factor of 0.5\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,         # Lower bound on the learning rate\n",
    "    verbose=1            # Print updates about learning rate reduction\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=25,\n",
    "    callbacks=[lr_scheduler] \n",
    ")\n",
    "\n",
    "model.evaluate(validation_dataset, verbose=2)\n",
    "\n",
    "model.export('rice_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "Model converted and saved to openvino_model/new_rice_model.xml and openvino_model/new_rice_model.bin\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "# Path to your SavedModel directory\n",
    "saved_model_path = \"rice_final\"\n",
    "\n",
    "ov_model = mo.convert_model(saved_model_path, \n",
    "                            model_name=\"rice_model\",\n",
    "                            framework=\"tf\", input_shape=[1, 224, 224, 3])  \n",
    "\n",
    "# Specify the output directory and filenames\n",
    "output_dir = \"openvino_model\"\n",
    "xml_filename = \"new_rice_model.xml\"\n",
    "bin_filename = \"new_rice_model.bin\"\n",
    "\n",
    "# Serialize the model to IR format\n",
    "serialize(ov_model, xml_filename, bin_filename)\n",
    "\n",
    "print(f\"Model converted and saved to {output_dir}/{xml_filename} and {output_dir}/{bin_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# parametros primera recomendacion y segunda\n",
    "crop = \"Cucumber\"\n",
    "disease = \"Bacterial Wilt\"\n",
    "\n",
    "recommendation1 = requests.post(\"https://fridaplatform.com/generate\", json={\"inputs\": f\"Provea 2 pequeñas observaciones de la planta {crop} a cuál crece actualmente en un invernadero con el estado de la planta predominante: (English state just for reference, yet answer in Spanish){disease}\", \"parameters\": {\"max_new_tokens\": 400, \"stream\": True}})\n",
    "\n",
    "\n",
    "print(recommendation1.json()['generated_text'])\n",
    "\n",
    "# parametros de la segunda recomendacion\n",
    "first_state = \"Healthy Leaf\"\n",
    "first_percentage = 60\n",
    "second_state = \"Blight Leaf\"\n",
    "second_percentage = 30\n",
    "third_state = \"Dark Leaf\"\n",
    "third_percentage = 10\n",
    "\n",
    "\n",
    "recommendation2 = requests.post(\"https://fridaplatform.com/generate\", json={\"inputs\": f\"Provea un calificación del 1 al 10 de en qué tan buen estado se encuentra un invernadero dado las siguientes estadísticas y estado de la planta: %{first_percentage} de la planta {crop} se encuentra como[utilizar nombre del estado en español para responder] {first_state}, %{second_percentage} se encuentra en la condición de {second_state}, y %{third_percentage} se encuentra en la condición de {third_state}. [Presente la respuesta como un único pequeño párrafo]\", \"parameters\": {\"max_new_tokens\": 300, \"stream\": True}})\n",
    "\n",
    "print(recommendation2.json()['generated_text'])\n",
    "\n",
    "# curl -X POST -H \"Content-Type: application/json\" -d '{\"inputs\":\"Help my friend write a python function for tensorflow model training\", \"parameters\":{\"max_new_tokens\":300, \"stream\":true}}' https://fridaplatform.com/generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
