{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3829 files belonging to 6 classes.\n",
      "Using 3064 files for training.\n",
      "Found 3829 files belonging to 6 classes.\n",
      "Using 765 files for validation.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:32:32.948683: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-09-14 19:32:32.948705: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-09-14 19:32:32.948716: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-09-14 19:32:32.948732: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-14 19:32:32.948742: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-09-14 19:32:33.518724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 91ms/step - accuracy: 0.2321 - loss: 1.8485 - val_accuracy: 0.4314 - val_loss: 1.3809 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 81ms/step - accuracy: 0.4686 - loss: 1.3751 - val_accuracy: 0.5190 - val_loss: 1.2445 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.5464 - loss: 1.2153 - val_accuracy: 0.5908 - val_loss: 1.0351 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.6349 - loss: 0.9901 - val_accuracy: 0.6314 - val_loss: 0.9439 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.6639 - loss: 0.8768 - val_accuracy: 0.6471 - val_loss: 0.8965 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.7155 - loss: 0.7512 - val_accuracy: 0.6706 - val_loss: 0.8986 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - accuracy: 0.7283 - loss: 0.7462 - val_accuracy: 0.7150 - val_loss: 0.7564 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - accuracy: 0.7921 - loss: 0.5879 - val_accuracy: 0.7399 - val_loss: 0.6904 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.7798 - loss: 0.5742 - val_accuracy: 0.7255 - val_loss: 0.8458 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.8145 - loss: 0.5224 - val_accuracy: 0.7582 - val_loss: 0.6822 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 66ms/step - accuracy: 0.8513 - loss: 0.4368 - val_accuracy: 0.7438 - val_loss: 0.7273 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - accuracy: 0.8812 - loss: 0.3536 - val_accuracy: 0.7686 - val_loss: 0.7374 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - accuracy: 0.8855 - loss: 0.3372 - val_accuracy: 0.7699 - val_loss: 0.6686 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9029 - loss: 0.2724 - val_accuracy: 0.7895 - val_loss: 0.6594 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.9058 - loss: 0.2552 - val_accuracy: 0.7843 - val_loss: 0.7591 - learning_rate: 0.0010\n",
      "24/24 - 0s - 15ms/step - accuracy: 0.7843 - loss: 0.7591\n",
      "INFO:tensorflow:Assets written to: rice_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rice_final/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'rice_final'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13656683536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13656685648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13656686032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13656686416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345519952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345520912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345520336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345521680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345522640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345523600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345523408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14345524176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "path = 'Rice_Leaf_AUG'\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32  \n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Store class_names before applying operations\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor the validation loss to adjust the learning rate\n",
    "    factor=0.5,          # Reduce the learning rate by a factor of 0.5\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,         # Lower bound on the learning rate\n",
    "    verbose=1            # Print updates about learning rate reduction\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=15,\n",
    "    callbacks=[lr_scheduler] \n",
    ")\n",
    "\n",
    "model.evaluate(validation_dataset, verbose=2)\n",
    "\n",
    "model.export('rice_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "Model converted and saved to openvino_model/rice_model.xml and openvino_model/rice_model.bin\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "# Path to your SavedModel directory\n",
    "saved_model_path = \"rice_final\"\n",
    "\n",
    "ov_model = mo.convert_model(saved_model_path, \n",
    "                            model_name=\"rice_model\",\n",
    "                            framework=\"tf\", input_shape=[1, 224, 224, 3])  \n",
    "\n",
    "# Specify the output directory and filenames\n",
    "output_dir = \"openvino_model\"\n",
    "xml_filename = \"rice_model.xml\"\n",
    "bin_filename = \"rice_model.bin\"\n",
    "\n",
    "# Serialize the model to IR format\n",
    "serialize(ov_model, xml_filename, bin_filename)\n",
    "\n",
    "print(f\"Model converted and saved to {output_dir}/{xml_filename} and {output_dir}/{bin_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
