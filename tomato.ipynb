{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25850 files belonging to 11 classes.\n",
      "Found 6683 files belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, Callback\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "train_dataset_path = 'train_tomato'\n",
    "validation_dataset_path = 'valid_tomato'\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dataset_path,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='int'\n",
    ")\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dataset_path,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 19:22:02.152714: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6190353686219760102\n",
      "2024-09-14 19:22:02.152732: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6952222542046581724\n",
      "2024-09-14 19:22:02.152736: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n",
      "\t [[{{node decode_image/DecodeImage}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[StatefulPartitionedCall/sequential_1_1/dropout_3_1/stateless_dropout/stateless_random_uniform/StatelessRandomGetKeyCounter/_78]]\n",
      "2024-09-14 19:22:02.152747: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 2914046606119890808\n",
      "2024-09-14 19:22:02.152755: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17855890933744541869\n",
      "2024-09-14 19:22:02.152761: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9347872040601936293\n",
      "2024-09-14 19:22:02.152764: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14644970230093890728\n",
      "2024-09-14 19:22:02.152782: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14116195550375633655\n",
      "2024-09-14 19:22:02.152799: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18275645724992468418\n",
      "2024-09-14 19:22:02.152811: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14718674831448405730\n",
      "2024-09-14 19:22:02.152814: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 3574135520661709579\n",
      "2024-09-14 19:22:02.152870: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11498756198104524191\n",
      "2024-09-14 19:22:02.152881: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1236240711430575644\n",
      "2024-09-14 19:22:02.152892: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18244965309404825966\n",
      "2024-09-14 19:22:02.152895: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16713624465183663565\n",
      "2024-09-14 19:22:02.152905: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5924170922492094399\n",
      "2024-09-14 19:22:02.152910: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17704363637158430209\n",
      "2024-09-14 19:22:02.152914: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 6597574064002271995\n",
      "2024-09-14 19:22:02.152916: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 17471259274331130416\n",
      "2024-09-14 19:22:02.152928: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 9242944633906948626\n",
      "2024-09-14 19:22:02.152932: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1070849890062284518\n",
      "2024-09-14 19:22:02.152935: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 1850226551976660666\n",
      "2024-09-14 19:22:02.152938: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12348062213116399346\n",
      "2024-09-14 19:22:02.152946: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12465393934069829993\n",
      "2024-09-14 19:22:02.152950: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 8503829543925191749\n",
      "2024-09-14 19:22:02.152952: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 18101869286371620359\n",
      "2024-09-14 19:22:02.152956: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12628261032119090497\n",
      "2024-09-14 19:22:02.152961: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 5736654847464120151\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/sequential_1_1/dropout_3_1/stateless_dropout/stateless_random_uniform/StatelessRandomGetKeyCounter/_78]]\n  (1) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_9634]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     48\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_logger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(validation_dataset, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/sequential_1_1/dropout_3_1/stateless_dropout/stateless_random_uniform/StatelessRandomGetKeyCounter/_78]]\n  (1) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_9634]"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class ImageLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if logs and logs.get('loss') is None:  # Check if loss is None, which might indicate a problem\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.text('Problematic Batch', f'Batch {batch} encountered an issue', step=batch)\n",
    "                # You can add more logging here, such as saving the actual images if accessible\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.writer.close()\n",
    "image_logger = ImageLogger(log_dir='./image_logs')\n",
    "\n",
    "# Build the model (same as before)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=8,\n",
    "    callbacks=[lr_scheduler, image_logger], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(validation_dataset, verbose=2)\n",
    "\n",
    "# Save the model\n",
    "model.save('tomato_final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering and rezising\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory tomatodataset/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 42\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# # Filter files in both training and validation directories\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# filter_image_formats(train_dataset_path)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# filter_image_formats(validation_dataset_path)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# resize_large_images(train_dataset_path)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# resize_large_images(validation_dataset_path)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone filtering and rezising\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[1;32m     49\u001b[0m     validation_dataset_path,\n\u001b[1;32m     50\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     51\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m class_names \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mclass_names\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/keras/src/utils/image_dataset_utils.py:224\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/keras/src/utils/dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory tomatodataset/train"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_large_images(directory, target_size=(224, 224)):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.png'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with Image.open(file_path) as img:\n",
    "                    if img.size[0] > target_size[0] or img.size[1] > target_size[1]:\n",
    "                        print(f\"Resizing large image: {file}\")\n",
    "                        img = img.resize(target_size)\n",
    "                        img.save(file_path)  # Overwrite the original file\n",
    "\n",
    "# Function to filter allowed image formats (JPEG, PNG, GIF, BMP)\n",
    "def filter_image_formats(directory):\n",
    "    allowed_extensions = ['jpeg', 'png', 'gif', 'bmp']\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not any(file.lower().endswith(ext) for ext in allowed_extensions):\n",
    "                print(f\"Removing unsupported file format: {file}\")\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "train_dataset_path = 'tomatodataset/train'\n",
    "validation_dataset_path = 'tomatodataset/valid'\n",
    "\n",
    "# # Filter files in both training and validation directories\n",
    "# filter_image_formats(train_dataset_path)\n",
    "# filter_image_formats(validation_dataset_path)\n",
    "\n",
    "# # Apply resizing to both training and validation directories\n",
    "# resize_large_images(train_dataset_path)\n",
    "# resize_large_images(validation_dataset_path)\n",
    "\n",
    "print(\"Done filtering and rezising\")\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dataset_path,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dataset_path,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Monitor the validation loss to adjust the learning rate\n",
    "    factor=0.5,          # Reduce the learning rate by a factor of 0.5\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,         # Lower bound on the learning rate\n",
    "    verbose=1            # Print updates about learning rate reduction\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=8,\n",
    "    callbacks=[lr_scheduler] \n",
    ")\n",
    "\n",
    "model.evaluate(validation_dataset, verbose=2)\n",
    "\n",
    "# model.save('rice_model.h5')  # Saves to HDF5 format\n",
    "model.export('tomato_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "Model converted and saved to openvino_model/tomato_model.xml and openvino_model/tomato_model.bin\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "# Path to your SavedModel directory\n",
    "saved_model_path = \"tomato_final\"\n",
    "\n",
    "ov_model = mo.convert_model(saved_model_path, \n",
    "                            model_name=\"tomato_model\",\n",
    "                            framework=\"tf\", input_shape=[1, 224, 224, 3])  \n",
    "\n",
    "# Specify the output directory and filenames\n",
    "output_dir = \"openvino_model\"\n",
    "xml_filename = \"tomato_model.xml\"\n",
    "bin_filename = \"tomato_model.bin\"\n",
    "\n",
    "# Serialize the model to IR format\n",
    "serialize(ov_model, xml_filename, bin_filename)\n",
    "\n",
    "print(f\"Model converted and saved to {output_dir}/{xml_filename} and {output_dir}/{bin_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n"
     ]
    },
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'is_conversion_successful' failed at src/frontends/tensorflow/src/frontend.cpp:479:\nFrontEnd API failed with OpConversionFailure:\n[TensorFlow Frontend] Internal error, no translator found for operation(s): ImageProjectiveTransformV3, StatelessRandomGetKeyCounter, StatelessRandomUniformV2\nTo facilitate the conversion of unsupported operations, refer to Frontend Extension documentation: https://docs.openvino.ai/latest/openvino_docs_Extensibility_UG_Frontend_Extensions.html \n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpConversionFailure\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Path to your SavedModel directory\u001b[39;00m\n\u001b[1;32m      6\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcucumber_final\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcucumber_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Specify the output directory and filenames\u001b[39;00m\n\u001b[1;32m     13\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvino_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/openvino/tools/mo/convert.py:364\u001b[0m, in \u001b[0;36mconvert_model\u001b[0;34m(input_model, help, framework, input, output, input_shape, example_input, batch, mean_values, scale_values, scale, reverse_input_channels, source_layout, target_layout, layout, compress_to_fp16, extensions, transform, transformations_config, silent, log_level, version, progress, stream_output, share_weights, example_output, input_model_is_text, input_checkpoint, input_meta_graph, saved_model_dir, saved_model_tags, tensorflow_custom_operations_config_update, tensorflow_object_detection_api_pipeline_config, tensorboard_logdir, tensorflow_custom_layer_libraries, input_symbol, nd_prefix_name, pretrained_model_name, save_params_from_nd, legacy_mxnet_model, enable_ssd_gluoncv, input_proto, caffe_parser_path, k, disable_omitting_optional, enable_flattening_nested_params, counts, remove_output_softmax, remove_memory, **args)\u001b[0m\n\u001b[1;32m    362\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(args)\n\u001b[1;32m    363\u001b[0m cli_parser \u001b[38;5;241m=\u001b[39m get_all_cli_parser()\n\u001b[0;32m--> 364\u001b[0m ov_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcli_parser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m restore_logger_state(logger_state)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/Documents/Coding Projects 2024/plants/venv/lib/python3.11/site-packages/openvino/tools/mo/convert_impl.py:947\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(cli_parser, framework, args, python_api_used)\u001b[0m\n\u001b[1;32m    945\u001b[0m send_conversion_result(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfail\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m python_api_used:\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, argv\n",
      "\u001b[0;31mOpConversionFailure\u001b[0m: Check 'is_conversion_successful' failed at src/frontends/tensorflow/src/frontend.cpp:479:\nFrontEnd API failed with OpConversionFailure:\n[TensorFlow Frontend] Internal error, no translator found for operation(s): ImageProjectiveTransformV3, StatelessRandomGetKeyCounter, StatelessRandomUniformV2\nTo facilitate the conversion of unsupported operations, refer to Frontend Extension documentation: https://docs.openvino.ai/latest/openvino_docs_Extensibility_UG_Frontend_Extensions.html \n\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "# Path to your SavedModel directory\n",
    "saved_model_path = \"cucumber_final\"\n",
    "\n",
    "ov_model = mo.convert_model(saved_model_path, \n",
    "                            model_name=\"cucumber_model\",\n",
    "                            framework=\"tf\", input_shape=[1, 224, 224, 3])  \n",
    "\n",
    "# Specify the output directory and filenames\n",
    "output_dir = \"openvino_model\"\n",
    "xml_filename = \"cucumber_model.xml\"\n",
    "bin_filename = \"cucumber_model.bin\"\n",
    "\n",
    "# Serialize the model to IR format\n",
    "serialize(ov_model, xml_filename, bin_filename)\n",
    "\n",
    "\n",
    "print(f\"Model converted and saved to {output_dir}/{xml_filename} and {output_dir}/{bin_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
